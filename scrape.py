from playwright.sync_api import sync_playwright
import pandas as pd
from datetime import datetime
import time
import logging
from config_path import keywords

logger = logging.getLogger("scrape")
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(name)s | %(message)s')
console_handler.setFormatter(formatter)

logger.addHandler(console_handler)


def search_news(keyword: str, max_pages: int = 5):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        search_url = f"https://www.google.com/search?q={keyword.replace(' ', '+')}&tbm=nws&tbs=sbd:1"
        page.goto(search_url)

        titles_links = []
        seen_titles = set()

        for page_num in range(1, max_pages + 1):
            logger.info(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà {page_num}")
            time.sleep(2)
            page.keyboard.press("End")
            time.sleep(2)

            titles = page.query_selector_all('div[role="heading"][aria-level="3"]')
            for title_el in titles:
                try:
                    title = title_el.text_content().strip()
                    a_tag = title_el.query_selector("xpath=ancestor::a")
                    link = a_tag.get_attribute("href") if a_tag else None

                    main_element = title_el.query_selector("xpath=..")
                    date_element = main_element.query_selector('[class="OSrXXb rbYSKb LfVVr"]')
                    date_text = None
                    if date_element:
                        date_text = date_element.text_content().strip()
                    else:
                        logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ Date")

                    if title and link and title not in seen_titles:
                        titles_links.append((title, link, date_text))
                        seen_titles.add(title)
                except Exception as e:
                    logger.error(f" Error ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page_num}: {e}")
                    continue

            next_button = page.query_selector('#pnnext')
            if next_button:
                try:
                    next_button.click(timeout=5000)
                    time.sleep(2)  # ‡∏£‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
                except Exception as e:
                    logger.warning(f" ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏î‡πâ: {e}")
                    break
            else:
                break

        browser.close()
        return titles_links


def save_to_csv(data, filename="raw_scrape.csv"):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    logger.info(f" ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df)} ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô {filename}")


def news_scraper():
    all_results = []

    for keyword in keywords:
        logger.info(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î: {keyword}")
        results = search_news(keyword)
        for title, link, date in results:
            all_results.append({
                "Fetched Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Keyword": keyword,
                "Title": title,
                "Link": link,
                "Date": date
            })

    save_to_csv(all_results)


if __name__ == "__main__":
    news_scraper()
